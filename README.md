# 网络爬虫

![index](./images/index.jpg)

## 1. 网络爬虫简介

> 理想情况下，网络爬虫并不是必需品，每个网站都应该提供`API`接口，以结构化的格式共享它们的数据。但是通常都会有限制，而且得到的数据有可能并不是我们期望得到的结果。

### 1.1 前期准备

`robots.txt`
- 建议爬虫的规范

`sitemap.xml`
- 检查网站地图，其中定义了网站的所以链接地址

估算网站大小
- 通过估算得到的数量来确定使用何种爬虫技术
- 通过浏览器的`site`参数估算网站的链接地址个数
- `site:example.webscraping.com/view`

识别网站所用的技术
- 通过`pip`安装第三方扩展`builtwith`来查询，`builtwith.parse('url')`

寻找网站所有者
- 通过`pip`安装第三方扩展`python-whois`来查询，`whois.whois('url')`


### 1.2 三种爬取方式

- 爬取网站地图
- 遍历每一个网页的数据库`ID`
- 追踪网页链接


### 1.3 高级特性

**(1)** 解析`robots.txt`
- `robotparser`模块中的`can_fetch`进行判断

**(2)** 支持代理
- `urllib2`模块
- `requests`模块

**(3)** 下载限速
- 时间戳记录访问时间，`time.sleep`进行限速

**(4)** 避免爬虫陷阱
- 记录深度，访问到当前网页经过了多少个链接



## 2. 数据抓取

> 抓取需求数据之前，需要我们对网页进行分析，利用浏览器的开发者工具对网页结构进行透彻的分析，这样可以方便我们后续爬取工作的进度。

### 2.1 网页抓取的方式

- 正则表达式
- `BeautifulSoup4`
- `Lxml`


### 2.2 `CSS`选择器

> 熟悉`jQuery`的一定很熟悉这样方式了

| 选择器 | 表示方式 |
| -------- | -------- |
| 选择所有标签 | `*` |
| 选择所有`<a>`标签 | `a` |
| 选择所有`class="link"`标签 | `.link` |
| 选择`class="link"`的`<a>`标签 | `a.link` |
| 选择`id="home"`的`<a>`标签 | `a#home` |
| 选择父元素为`<a>`标签的所有`<span>`子标签 | `a > span` |
| 选择`<a>`标签内部的所有`<span>`标签 | `a span` |
| 选择`title`属性为`"Home"`的所有`<a>`标签 | `a[title=Home]` |


### 2.3 性能对比

| 抓取方法 | 性能 | 使用难度 | 安装难度 |
| :-------- | :-------- | :------ | :------ |
| 正则表达式 | 快 | 困难 | 简单(内置模块) |
| `BeautifulSoup4` | 慢 | 简单 | 简单(纯Python) |
| `Lxml` | 快 | 简单 | 困难 |




## 3. 下载缓存

> 为了方便后续的使用以及资源的整合，我们需要对链接爬虫**添加缓存**的支持。当使用上缓存之后，就能够灵活的控制网页的爬虫质量。

### 3.1 磁盘缓存

**简述**

基于磁盘的缓存系统比较容易实现，无需安装其他模块，并且在文件管理器中就能查看结果。

避免这些限制的一种解决方案是使用 `URL` 的 哈希值作为文件名 。 尽管该方法可以带来一定改善 ，但是最终还是会面临许多文件系统具有的一个关键问题 ，那就是每个卷和每个目录下的文件数量是有限制 的 。如果缓存存储在`FAT32`文件系统中 ，每个目录的最大文件数是`65535` 。该限制可以通过将缓存分割到不同目录来避免 ，但是文件系统可存储 的文件总数也是有限制的 。我使用的`ext4`分区目前支持略多于`1500`万个文件，而一个大型网站往往拥有超过1亿个网页 。很遗憾 ，`DiskCache` 方法想要通用的话存在太多限制 。要想避免这些问题 ， 我们需要把多个缓存网页合并到一个文件中 ，并使用类似 `B＋`树的算法进行索引 。我们并不会自己实现这种算法 ，而是使用数据库缓存解决这个问题 。


### 3.2 数据库缓存

**NoSQL**
- `HBase`
 - 列数据存储
- `Redis`
 - 键值对存储
- `MongoDB`
 - 面向文档的数据库
- `Neo4j`
 - 图形数据库


**安装`MongoDB`**

```bash
# 安装MongoDb之后，在安装python的MongoDB封装库
$ pip install pymongo

# 启动本地MongoDB
$ mongod -dbpath .

# python连接MongoDB
>>> from pymongo import MongoClient
>>> client = MongoClient('localhost', 27017)
```


## 4. 并发下载

### 4.1 获取`URLS`链接地址

### 4.2 串行爬虫

### 4.3 多线程爬虫

### 4.4 多进程和多线程爬虫


## 5. 动态内容

> 和单页面应用的**简单表单事件**不同，使用`JavaScript`时，不再是加载后立即下载所有页面内容。这样就会造成许多网页在浏览器中展示的内容不会出现在`HTML`源代码中，本书前面介绍的抓取技术也就无法正常运转了 。所以这里我们采取两种方式进行抓取。
> - **`JavaScript`逆向工程**
> - **渲染`JavaScript`**

### 5.1 `AJAX`异步请求

> **`AJAX`**是指异步`JavaScript`和`XML`，描述了一种跨浏览器动态生成`Web`应用内容的功能。

1. 该技术允许`JavaScript`创建到远程服务器`HTTP`请求并获得响应，也就是说 `Web` 应用就可以传输和接收数据
2. 而传统的客户端与服务端交互方式则是刷新整个网页，这种方式的用户体验比较差，并且在只需传输少量数据时会造成带宽浪费。
3. 重要的实现函数为`XMLHttpRequest`方法。



### 5.2 `JavaScript`逆向工程

> **逆向工程: **该网页中的数据是使用 `JavaScript` 动态加载的，所以要想抓取该数据就需要了解网页是如何加载该数据的，该过程也被称为逆向工程。

**抓取方法**

- 在 `Firebug` 中单击 `Console` 选项卡，然后执行一次搜索，我们将会看到产生了一个请求信息。
- 请求的`URL`为`http://example.webscraping.com/places/ajax/search.json?&search_term=A&page_size=10&page=0`


#### 5.2.1 尝试匹配

- 由请求的`URL`可以知道参数为`search_term=`/`page_size=`/`page=`，对其进行匹配下载


#### 5.2.2 边界情况

- 对其参数边界的尝试，使分为多次的下载变为一次执行
- `search_term＝.`
- `page_size=1OOO`


### 5.3 渲染`JavaScript`

> 对于非常复杂的网站，我们很难实施逆向工程分析，及时使用了`Firebug`也很难进行理解。加之很多网站使用了`Google Web Toolkit(GWT)`技术进行开发，使得`JavaScript`代码是机器生成的压缩版。

- 我们可以使用**浏览器渲染引擎**避免这些工作，这种渲染引擎是浏览器在显示 网页时解析 `HTML`、应用 `css` 样式并执行 `JavaScript` 语句的部分 。
- 不同的浏览器有自己不同的渲染引擎，这里我们使用的`WebKit`。


#### 5.3.1 第三方库使用

- `Python`中解析渲染引擎的第三方库有`PyQt`/`PySide`等。


#### 5.3.2 `WebKit`与网站交互

> **交互：**实例类似于表单一样，将需要的信息动态的提交给网站。

- 实现`WebKit`爬虫最难得部分就是抓取搜索结果，因为很难预计结果的输出，所以这里我们有三种方式来处理这个问题。

**等待结果**

- 等待一定时间，期望`AJAX`事件能够在此时刻之前完成
 - **特点：**最易实现、效率低
- 重写`Qt`的网络管理器，跟踪`URL`请求的完成时间
 - **特点：**效率高、延迟出现在客户端则无法使用
- **轮询**网页等待特定内容出现
 - **特点：**更加可靠且易实现、检查内容是否加载完成时浪费`CPU`周期


**渲染类**

- 为了提升能后续的易用性，下面会把使用到的方法封装到一个类中。


### 5.4 Selenium

> 前面使用`WebKit`库，我们可以灵活的指定浏览器的渲染引擎。但是如果不需要这么完全控制所以行为的话，可以使用`Selenium`作为替代品，它提供了浏览器自动化的`API`接口。


### 5.5 总结

- 浏览器渲染引擎能够为我们节省了解网站后端工作原理的时间，但是该方法也有其劣势。
- 渲染网页增加了开销，使其比单纯下载 `HTML` 更慢。
- 另外，使用浏览器渲染引擎的方法通常需要轮询网页来检查是否已经得到事件生成的 `HTML` ，这种方式非常脆弱，在网络较慢时会经常会失败。
- 我一般将**浏览器渲染引擎作为短期解决方案**，此时长期的性能和可靠性并不算重要 ： 而作为**长期解决方案**需要尽最大努力对网站进行**逆向工程**。



