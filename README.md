# 网络爬虫


## 1. 网络爬虫简介

> 理想情况下，网络爬虫并不是必需品，每个网站都应该提供`API`接口，以结构化的格式共享它们的数据。但是通常都会有限制，而且得到的数据有可能并不是我们期望得到的结果。

### 1.1 前期准备

`robots.txt`
- 建议爬虫的规范

`sitemap.xml`
- 检查网站地图，其中定义了网站的所以链接地址

估算网站大小
- 通过估算得到的数量来确定使用何种爬虫技术
- 通过浏览器的`site`参数估算网站的链接地址个数
- `site:example.webscraping.com/view`

识别网站所用的技术
- 通过`pip`安装第三方扩展`builtwith`来查询，`builtwith.parse('url')`

寻找网站所有者
- 通过`pip`安装第三方扩展`python-whois`来查询，`whois.whois('url')`


### 1.2 三种爬取方式

- 爬取网站地图
- 遍历每一个网页的数据库`ID`
- 追踪网页链接


### 1.3 高级特性

**(1)** 解析`robots.txt`
- `robotparser`模块中的`can_fetch`进行判断

**(2)** 支持代理
- `urllib2`模块
- `requests`模块

**(3)** 下载限速
- 时间戳记录访问时间，`time.sleep`进行限速

**(4)** 避免爬虫陷阱
- 记录深度，访问到当前网页经过了多少个链接



## 2. 数据抓取

> 抓取需求数据之前，需要我们对网页进行分析，利用浏览器的开发者工具对网页结构进行透彻的分析，这样可以方便我们后续爬取工作的进度。

### 2.1 网页抓取的方式

- 正则表达式
- `BeautifulSoup4`
- `Lxml`


### 2.2 `CSS`选择器

> 熟悉`jQuery`的一定很熟悉这样方式了

| 选择器 | 表示方式 |
| -------- | -------- |
| 选择所有标签 | `*` |
| 选择所有`<a>`标签 | `a` |
| 选择所有`class="link"`标签 | `.link` |
| 选择`class="link"`的`<a>`标签 | `a.link` |
| 选择`id="home"`的`<a>`标签 | `a#home` |
| 选择父元素为`<a>`标签的所有`<span>`子标签 | `a > span` |
| 选择`<a>`标签内部的所有`<span>`标签 | `a span` |
| 选择`title`属性为`"Home"`的所有`<a>`标签 | `a[title=Home]` |


### 2.3 性能对比

| 抓取方法 | 性能 | 使用难度 | 安装难度 |
| :-------- | :-------- | :------ | :------ |
| 正则表达式 | 快 | 困难 | 简单(内置模块) |
| `BeautifulSoup4` | 慢 | 简单 | 简单(纯Python) |
| `Lxml` | 快 | 简单 | 困难 |

